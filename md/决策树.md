# 决策树

* 什么是决策树？
* 决策树的主要步骤？
* 决策树算法？

***

这里先给出一个需要被决策的简单例子，并且给定数据，可以先跳过。

| 课堂活跃度 | 出勤率（%） | 作业完成率（%） | 最终成绩是否优异 |
| ---------- | ----------- | --------------- | ---------------- |
| 70         | 100         | 85              | 是               |
| 90         | 95          | 95              | 是               |
| 20         | 70          | 50              | 否               |



## 什么是决策树？

顾名思义，决策树是用来做决策的。通过从给定数据中提取特征并形成一棵树的方式来做出决策。树的叶节点就是决策结果，而非叶节点是决策条件。

**注意**：我们要从已知数据中*提取特征* *构建模型*并且使得模型具有泛化能力，也就是说**特征是人为提取的**，是给定数据中应该包含的，决策树要做的是**计算合适的分界线**，比如作业完成率高于80%就判断此学生最终成绩优异

所以之后提及的决策树算法既要能够**精准决策**，又要能够具有足够的**泛化能力**。

## 决策树的主要步骤？

1. 节点的分裂：当前节点无法给出判断时将当前节点分裂成几个节点
2. 阈值的选择：选择适当的阈值使得分类错误率最小

## 决策树算法？

### ID3

增熵公式：


$$
H=-\sum{p_i*log(p_i)}
$$

$其中p_i为每一类出现的概率，H为熵$

熵为自信息的期望，代表了当前系统的混乱程度，熵越小（接近0）则混乱程度越低，熵越大（接近1）则混乱程度越大。

可以认为在决策未开始时我们拥有一个较大的熵（经验熵，比如认为两类各占50%）（因为我们什么也不知道），每做出一次决策时，我们要求熵要减小（也就是我们知道了一些信息可以做出决策），并且要求熵的减少量要最大，这样就可以最快的到达熵为0的状态，也就是说我们可以确定这个学生最终成绩优异与否了（因为只剩下了一类，无法再分，决策已然完成）。

但是ID3有一个问题：分割过细。也就是所谓的过拟合。想象一下，如果我们直接将作业完成率阈值设定为大于80小于85就判断成绩优异，是不是也可以？但是这样的话虽然在训练数据上符合了条件，但是我们训练决策树并不是要对训练数据做分类的，而是要预测未知数据。

### C4.5

在ID3的的基础上增加了信息增益比，简单来说，如果分割过于精细，信息增益比就会下降，导致虽然该选择熵减较小但是我们并不会做出如此选择。

以上两个算法在同一个特征只会被判断一次，可以在创建多个分支。而CART则可以多次利用同一特征并且只进行二分处理。

### CART

CART使用Gini系数取代了熵
$$
Gini=\sum p_i(1-p_i)
$$
同时，递归的处理每个分支，直到满足一定条件（如均为一类或小于阈值）

CART可以做回归处理，即输出一个实数而不是一个类别。

剪枝：CART可能会出现递归深度过深、过拟合的问题，所以对于一些不太好的分支可以直接剪掉。

如何剪枝？

代价复杂性剪枝：

大体上来说是生成所有被剪枝的子树然后做判断的方案，感兴趣的可以参照下图和所附链接：

![20180117145256927](D:\Projects\Waitti-long.github.io\md\决策树.assets\20180117145256927.png)

> 图片来源：https://blog.csdn.net/zhengzhenxian/article/details/79083643

> 参考：
>
> https://www.cnblogs.com/xmeo/p/6543054.html
>
> https://zhuanlan.zhihu.com/p/30059442
>
> https://www.cnblogs.com/xiemaycherry/p/10475067.html
>
> https://www.cnblogs.com/keye/p/10564914.html